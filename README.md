# Liquidity Key Levels RL Trader ğŸš€

A research project exploring a reinforcement-learning (RL) trading agent that â€œseesâ€ both price action **and** options-derived liquidity key levels. The agent learns to trade SPY using a Deep Q-Learning (DQN) policy with risk-aware position management.

> **Disclaimer:** This project is for research and educational purposes only. **Not financial advice.** ğŸ“  
> **Data availability:** You **wonâ€™t find all datasets in this repo** because of their large size. Use the collection scripts with your own API keys to rebuild the key tables. ğŸ“¦

---

## Chapter 1 â€” Data Collection ğŸ“Š

We assembled three complementary datasets:

- **Options data (Alpha Vantage API):**  
  Collected and consolidated into a single large file. Focus on open interest (OI), expirations, and strikes to infer weekly liquidity levels.

- **Price data (SPY):**  
  Gathered first from **TFT Broker**, then with **Barchart** data. After cleaning and deduplication, we organized everything into a unified price time series.

- **News articles (CNBC):**  
  Scraped and stored for later **market sentiment analysis** to contextualize regime shifts and high-volatility periods.

---

## Chapter 2 â€” Data Processing ğŸ§¹

We processed options and price data **in sync** to build model-ready features:

- **Weekly options key levels (statistical + analytical) ğŸ§ ğŸ“:**  
  Identified using a **smart statistical edge on contract open interest (OI)** **combined with analytical analyses** each **week** using the **monthly expiration** dates to confirm meaningful support/resistance zones.

- **Weekly Expected Move (EM):**  
  Computed and aligned with the trading horizon to set volatility-aware targets/bounds.

- **Gamma Exposure (GEX) key levels:**  
  Derived to capture dealer-flow dynamics and likely support/resistance zones.

- **Final feature table:**  
  All signalsâ€”price OHLCV, OI levels, EM and GEX levels were merged into a **single table** indexed by timestamp, ready for exploration by the model.

---

## Chapter 3 â€” Reinforcement Learning Model ğŸ§ 

After experimenting with several RL environments, we converged on a DQN-based agent that performed best in our tests:

- **Action space:** `Buy`, `Sell`, `Hold`, plus **position adjustments** (add, trim, or fully exit).   
- **Risk management:** Smart sizing and safeguards to control drawdowns and tail risk.  
- **Why DQN?** DQNâ€™s deep neural network approximator adapts better to this **high-dimensional, non-linear** setting than a simple Q-table.

---

